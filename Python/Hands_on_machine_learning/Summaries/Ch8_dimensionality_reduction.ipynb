{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality reduction\n",
    "## &nbsp; The curse of Dimensionality\n",
    "* If you pick two points randomly in a unit square, the distance between the points would be roughly 0.52 on average.\n",
    "* Similarly, two random points in a unit cube (3D), the average distance would be about 0.66.\n",
    "* But in 1,000,000-dimensional hypercube, the average distance would be about 408.25, meaning the points are so far  \n",
    "apart when they both lie within the same unit hypercube.\n",
    "* As a result, high-dimensional datasets are at risk of being very sparse, meaning the instances in the training set  \n",
    "are far away from each other.\n",
    "* This also means that when a new instance comes in, it would be far away from the other instances also, making the  \n",
    "prediction on the instance less reliable than in lower dimensions.\n",
    "* To summarize, in higher dimensional data, there is a greater risk of overfitting.\n",
    "* One solution is to increase the size of the dataset to reach a sufficient density of training instances but the  \n",
    "number of intances required grows exponentially with the number of dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## &nbsp; Main approaches for dimensionality reduction\n",
    "### &nbsp;&nbsp;&nbsp; Projection  \n",
    "* In practice, many features are almost constant, while others are highly correlated (like in MNIST). \n",
    "* As a result, the training instances lie within a much lower-dimensional subspace.\n",
    "* If we project the training instances(e.g. in 3-D) perpendicularly onto a subspace(2-D), we'd get the new 2-D  \n",
    "dataset, with new features(axes)\n",
    "* But this is not always the best approach as the subspace may twist and turn like 'swiss roll'\n",
    "* Simply projecting onto a plane would just squash different layers of the swiss roll together.\n",
    "### &nbsp;&nbsp;&nbsp; Manifold learning  \n",
    "* Swiss roll is an example of a 2D manifold. It is a 2D shape that can be bent and twisted in a higher-dimensional space.\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}